{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enat/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib  # To load saved models\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase_value    float64\n",
      "device_id          object\n",
      "source             object\n",
      "browser            object\n",
      "sex                object\n",
      "age               float64\n",
      "signup_year       float64\n",
      "signup_month      float64\n",
      "signup_day        float64\n",
      "signup_hour       float64\n",
      "purchase_year     float64\n",
      "purchase_month    float64\n",
      "purchase_day      float64\n",
      "purchase_hour     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load fraud data\n",
    "fraud_data = pd.read_csv('Fraud_Data.csv')\n",
    "ip_data = pd.read_csv('IpAddress_to_Country.csv')\n",
    "\n",
    "# Load credit card data\n",
    "creditcard_data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# Create date-related features\n",
    "fraud_data['signup_time'] = pd.to_datetime(fraud_data['signup_time'])\n",
    "fraud_data['purchase_time'] = pd.to_datetime(fraud_data['purchase_time'])\n",
    "\n",
    "fraud_data['signup_year'] = fraud_data['signup_time'].dt.year\n",
    "fraud_data['signup_month'] = fraud_data['signup_time'].dt.month\n",
    "fraud_data['signup_day'] = fraud_data['signup_time'].dt.day\n",
    "fraud_data['signup_hour'] = fraud_data['signup_time'].dt.hour\n",
    "\n",
    "fraud_data['purchase_year'] = fraud_data['purchase_time'].dt.year\n",
    "fraud_data['purchase_month'] = fraud_data['purchase_time'].dt.month\n",
    "fraud_data['purchase_day'] = fraud_data['purchase_time'].dt.day\n",
    "fraud_data['purchase_hour'] = fraud_data['purchase_time'].dt.hour\n",
    "\n",
    "# Focus on essential columns and remove potential problematic ones\n",
    "columns_to_keep = ['purchase_value', 'device_id', 'source', 'browser', 'sex', 'age', 'signup_year', 'signup_month', 'signup_day', 'signup_hour', 'purchase_year', 'purchase_month', 'purchase_day', 'purchase_hour']\n",
    "\n",
    "# Create a new dataframe with the selected columns\n",
    "fraud_data_clean = fraud_data[columns_to_keep]\n",
    "\n",
    "# Explicitly convert categorical columns to strings\n",
    "categorical_cols = ['device_id', 'source', 'browser', 'sex']\n",
    "fraud_data_clean[categorical_cols] = fraud_data_clean[categorical_cols].astype(str)\n",
    "\n",
    "# Convert numerical columns to floats\n",
    "numerical_cols = ['purchase_value', 'age', 'signup_year', 'signup_month', 'signup_day', 'signup_hour', 'purchase_year', 'purchase_month', 'purchase_day', 'purchase_hour']\n",
    "fraud_data_clean[numerical_cols] = fraud_data_clean[numerical_cols].astype(float)\n",
    "\n",
    "# Handle potential missing values (optional but recommended)\n",
    "fraud_data_clean.fillna(0, inplace=True)\n",
    "\n",
    "# Verify the columns and their types\n",
    "print(fraud_data_clean.dtypes)\n",
    "\n",
    "# Define column transformer with one-hot encoding for categorical features and scaling for numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', StandardScaler(), numerical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Separate features and target\n",
    "X_fraud = fraud_data_clean\n",
    "y_fraud = fraud_data['class']\n",
    "\n",
    "# Transform the features\n",
    "X_fraud_transformed = preprocessor.fit_transform(X_fraud)\n",
    "\n",
    "# Standardize credit card data\n",
    "X_creditcard = StandardScaler().fit_transform(creditcard_data.drop(columns=['Class']).values)\n",
    "y_creditcard = creditcard_data['Class']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_fraud_train, X_fraud_test, y_fraud_train, y_fraud_test = train_test_split(X_fraud_transformed, y_fraud, test_size=0.2, random_state=42)\n",
    "X_creditcard_train, X_creditcard_test, y_creditcard_train, y_creditcard_test = train_test_split(X_creditcard, y_creditcard, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Paths to your saved models\n",
    "logistic_regression_model_path = 'mlruns/901575089113022840/ff9e67a75ca546cc9ed7f1c819e8f834/artifacts/Logistic Regression/model.pkl'\n",
    "random_forest_model_path = 'mlruns/901575089113022840/46bc3d1e00ce4070bd98466edca7034e/artifacts/Random Forest/model.pkl'\n",
    "gradient_boosting_model_path = 'mlruns/901575089113022840/e58badcec43a4e06b8ba78efc3c626d4/artifacts/Gradient Boosting/model.pkl'\n",
    "\n",
    "# Load models\n",
    "logistic_regression_model = joblib.load(logistic_regression_model_path)\n",
    "random_forest_model = joblib.load(random_forest_model_path)\n",
    "gradient_boosting_model = joblib.load(gradient_boosting_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "# Use a sample of the data to avoid memory issues\n",
    "sample_size = 1000  # Adjust sample size based on available memory\n",
    "X_fraud_train_sample = X_fraud_train[:sample_size].toarray() if scipy.sparse.issparse(X_fraud_train) else X_fraud_train[:sample_size]\n",
    "X_fraud_test_sample = X_fraud_test[:sample_size].toarray() if scipy.sparse.issparse(X_fraud_test) else X_fraud_test[:sample_size]\n",
    "\n",
    "# Assuming feature names are available\n",
    "feature_names = ['purchase_value', 'age', 'signup_year', 'signup_month', 'signup_day', 'signup_hour', 'purchase_year', 'purchase_month', 'purchase_day', 'purchase_hour'] + list(preprocessor.named_transformers_['cat'].get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LIME explainer for the Logistic Regression model\n",
    "explainer_lr = LimeTabularExplainer(X_fraud_train_sample,\n",
    "                                    mode='classification', \n",
    "                                    feature_names=feature_names,  \n",
    "                                    class_names=['Not Fraud', 'Fraud'], \n",
    "                                    discretize_continuous=True)\n",
    "\n",
    "# Explain a single prediction for Logistic Regression\n",
    "i = 0  # Index of the instance to explain\n",
    "exp_lr = explainer_lr.explain_instance(X_fraud_test_sample[i], logistic_regression_model.predict_proba, num_features=10)\n",
    "\n",
    "# Feature importance plot for Logistic Regression\n",
    "exp_lr.show_in_notebook(show_table=True)\n",
    "\n",
    "# Create a LIME explainer for the Random Forest model\n",
    "explainer_rf = LimeTabularExplainer(X_fraud_train_sample,\n",
    "                                    mode='classification', \n",
    "                                    feature_names=feature_names,  \n",
    "                                    class_names=['Not Fraud', 'Fraud'], \n",
    "                                    discretize_continuous=True)\n",
    "\n",
    "# Explain a single prediction for Random Forest\n",
    "exp_rf = explainer_rf.explain_instance(X_fraud_test_sample[i], random_forest_model.predict_proba, num_features=10)\n",
    "\n",
    "# Feature importance plot for Random Forest\n",
    "exp_rf.show_in_notebook(show_table=True)\n",
    "\n",
    "# Create a LIME explainer for the Gradient Boosting model\n",
    "explainer_gb = LimeTabularExplainer(X_fraud_train_sample,\n",
    "                                    mode='classification', \n",
    "                                    feature_names=feature_names,  \n",
    "                                    class_names=['Not Fraud', 'Fraud'], \n",
    "                                    discretize_continuous=True)\n",
    "\n",
    "# Explain a single prediction for Gradient Boosting\n",
    "exp_gb = explainer_gb.explain_instance(X_fraud_test_sample[i], gradient_boosting_model.predict_proba, num_features=10)\n",
    "\n",
    "# Feature importance plot for Gradient Boosting\n",
    "exp_gb.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assume you have already preprocessed your data and split into training and test sets\n",
    "# X_fraud_train_sample, X_fraud_test_sample, y_fraud_train, y_fraud_test, feature_names\n",
    "# For the sake of example, I'll create dummy data here\n",
    "np.random.seed(0)\n",
    "X_fraud_train_sample = np.random.rand(100, 10)\n",
    "X_fraud_test_sample = np.random.rand(10, 10)\n",
    "y_fraud_train = np.random.randint(2, size=100)\n",
    "y_fraud_test = np.random.randint(2, size=10)\n",
    "feature_names = [f'Feature {i}' for i in range(10)]\n",
    "\n",
    "# Create a Logistic Regression model and fit it to the data\n",
    "logistic_regression_model = LogisticRegression()\n",
    "logistic_regression_model.fit(X_fraud_train_sample, y_fraud_train)\n",
    "\n",
    "# Create a LIME explainer for the Logistic Regression model\n",
    "explainer_lr = LimeTabularExplainer(X_fraud_train_sample,\n",
    "                                    mode='classification', \n",
    "                                    feature_names=feature_names,  \n",
    "                                    class_names=['Not Fraud', 'Fraud'], \n",
    "                                    discretize_continuous=True)\n",
    "\n",
    "# Explain a single prediction for Logistic Regression\n",
    "i = 0  # Index of the instance to explain\n",
    "exp_lr = explainer_lr.explain_instance(X_fraud_test_sample[i], logistic_regression_model.predict_proba, num_features=10)\n",
    "\n",
    "# Save the explanation to an HTML file\n",
    "exp_lr.save_to_file('explanation_logistic_regression.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assume you have already preprocessed your data and split into training and test sets\n",
    "# X_fraud_train_sample, X_fraud_test_sample, y_fraud_train, y_fraud_test, feature_names\n",
    "# For the sake of example, I'll create dummy data here\n",
    "np.random.seed(0)\n",
    "X_fraud_train_sample = np.random.rand(100, 10)\n",
    "X_fraud_test_sample = np.random.rand(10, 10)\n",
    "y_fraud_train = np.random.randint(2, size=100)\n",
    "y_fraud_test = np.random.randint(2, size=10)\n",
    "feature_names = [f'Feature {i}' for i in range(10)]\n",
    "\n",
    "# Create a Random Forest model and fit it to the data\n",
    "random_forest_model = RandomForestClassifier()\n",
    "random_forest_model.fit(X_fraud_train_sample, y_fraud_train)\n",
    "\n",
    "# Create a LIME explainer for the Random Forest model\n",
    "explainer_rf = LimeTabularExplainer(X_fraud_train_sample,\n",
    "                                    mode='classification', \n",
    "                                    feature_names=feature_names,  \n",
    "                                    class_names=['Not Fraud', 'Fraud'], \n",
    "                                    discretize_continuous=True)\n",
    "\n",
    "# Explain a single prediction for Random Forest\n",
    "i = 0  # Index of the instance to explain\n",
    "exp_rf = explainer_rf.explain_instance(X_fraud_test_sample[i], random_forest_model.predict_proba, num_features=10)\n",
    "\n",
    "# Save the explanation to an HTML file\n",
    "exp_rf.save_to_file('explanation_random_forest.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Assume you have already preprocessed your data and split into training and test sets\n",
    "# X_fraud_train_sample, X_fraud_test_sample, y_fraud_train, y_fraud_test, feature_names\n",
    "# For the sake of example, I'll create dummy data here\n",
    "np.random.seed(0)\n",
    "X_fraud_train_sample = np.random.rand(100, 10)\n",
    "X_fraud_test_sample = np.random.rand(10, 10)\n",
    "y_fraud_train = np.random.randint(2, size=100)\n",
    "y_fraud_test = np.random.randint(2, size=10)\n",
    "feature_names = [f'Feature {i}' for i in range(10)]\n",
    "\n",
    "# Create a Gradient Boosting model and fit it to the data\n",
    "gradient_boosting_model = GradientBoostingClassifier()\n",
    "gradient_boosting_model.fit(X_fraud_train_sample, y_fraud_train)\n",
    "\n",
    "# Create a LIME explainer for the Gradient Boosting model\n",
    "explainer_gb = LimeTabularExplainer(X_fraud_train_sample,\n",
    "                                    mode='classification', \n",
    "                                    feature_names=feature_names,  \n",
    "                                    class_names=['Not Fraud', 'Fraud'], \n",
    "                                    discretize_continuous=True)\n",
    "\n",
    "# Explain a single prediction for Gradient Boosting\n",
    "i = 0  # Index of the instance to explain\n",
    "exp_gb = explainer_gb.explain_instance(X_fraud_test_sample[i], gradient_boosting_model.predict_proba, num_features=10)\n",
    "\n",
    "# Save the explanation to an HTML file\n",
    "exp_gb.save_to_file('explanation_gradient_boosting.html')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

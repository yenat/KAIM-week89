{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier  # Faster alternative to Gradient Boosting\n",
    "from sklearn.neural_network import MLPClassifier  # For MLP\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "fraud_data = pd.read_csv('Fraud_Data.csv')\n",
    "creditcard_data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# Reduce dataset size for testing\n",
    "fraud_data = fraud_data.sample(frac=0.1, random_state=42)  # Use 10% of the data\n",
    "creditcard_data = creditcard_data.sample(frac=0.1, random_state=42)  # Use 10% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date and time fields to datetime\n",
    "fraud_data['signup_time'] = pd.to_datetime(fraud_data['signup_time'])\n",
    "fraud_data['purchase_time'] = pd.to_datetime(fraud_data['purchase_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime fields to numerical features\n",
    "fraud_data['signup_year'] = fraud_data['signup_time'].dt.year\n",
    "fraud_data['signup_month'] = fraud_data['signup_time'].dt.month\n",
    "fraud_data['signup_day'] = fraud_data['signup_time'].dt.day\n",
    "fraud_data['signup_hour'] = fraud_data['signup_time'].dt.hour\n",
    "fraud_data['purchase_year'] = fraud_data['purchase_time'].dt.year\n",
    "fraud_data['purchase_month'] = fraud_data['purchase_time'].dt.month\n",
    "fraud_data['purchase_day'] = fraud_data['purchase_time'].dt.day\n",
    "fraud_data['purchase_hour'] = fraud_data['purchase_time'].dt.hour\n",
    "\n",
    "# Drop the original datetime columns\n",
    "fraud_data = fraud_data.drop(columns=['signup_time', 'purchase_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = ['device_id', 'source', 'browser', 'sex']  # Include 'device_id'\n",
    "\n",
    "# Define column transformer with one-hot encoding for categorical features and scaling for numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', StandardScaler(), ['purchase_value', 'age', 'signup_year', 'signup_month', 'signup_day', 'signup_hour', 'purchase_year', 'purchase_month', 'purchase_day', 'purchase_hour'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the features\n",
    "X_fraud = preprocessor.fit_transform(fraud_data.drop(columns=['class']))\n",
    "X_creditcard = StandardScaler().fit_transform(creditcard_data.drop(columns=['Class']).values)\n",
    "y_fraud = fraud_data['class']\n",
    "y_creditcard = creditcard_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_fraud_train, X_fraud_test, y_fraud_train, y_fraud_test = train_test_split(X_fraud, y_fraud, test_size=0.1, random_state=42)  # Use 10% test size\n",
    "X_creditcard_train, X_creditcard_test, y_creditcard_train, y_creditcard_test = train_test_split(X_creditcard, y_creditcard, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50),  # Reduce number of trees\n",
    "    'XGBoost': XGBClassifier(),  # Faster alternative to Gradient Boosting\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42),  # Simpler MLP\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate models\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and log models\n",
    "def train_and_log_model(model_name, model, X_train, X_test, y_train, y_test):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Train and evaluate on fraud data\n",
    "        acc_fraud, precision_fraud, recall_fraud, f1_fraud = train_and_evaluate(model, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Log metrics for fraud data\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_metric(\"accuracy_fraud\", acc_fraud)\n",
    "        mlflow.log_metric(\"precision_fraud\", precision_fraud)\n",
    "        mlflow.log_metric(\"recall_fraud\", recall_fraud)\n",
    "        mlflow.log_metric(\"f1_fraud\", f1_fraud)\n",
    "        \n",
    "        # Create an input example and convert to dense array\n",
    "        input_example = np.array(X_train[0].todense()).reshape(1, -1)\n",
    "        \n",
    "        # Log model with input example\n",
    "        mlflow.sklearn.log_model(model, model_name, input_example=input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MLflow tracking setup\n",
    "mlflow.set_experiment(\"Fraud Detection Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, evaluate, and log models\n",
    "with mlflow.start_run():\n",
    "    Parallel(n_jobs=-1)(delayed(train_and_log_model)(model_name, model, X_fraud_train, X_fraud_test, y_fraud_train, y_fraud_test) for model_name, model in models.items())\n",
    "\n",
    "print(\"Model training and evaluation complete. Check MLflow for detailed metrics and logs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
